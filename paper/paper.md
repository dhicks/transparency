---
title: "Title"
author: Daniel J. Hicks
date: Typeset \today
abstract: |
	abstract

<!--bibliography: ../bibfile.bib-->
---

# Introduction #

# Results #

## Validation of Values in Science Scale ##

To examine the underlying dimensionality of the Values in Science scale, we split the sample in half for the purposes of carrying out exploratory factor analysis (EFA) to see what underlying factor structure may be present based on participants' response and confirmatory factor analysis (CFA) to compare models that could be abstracted from the EFA. We created a dummy variable that randomly assigned participants to one of two groups, resulting in two sub-samples of 439 and 467 response sets, respectively. Before conducting the EFA, we checked whether the assumptions necessary for a valid EFA held. Bartlett's test of sphericity was significant, $$\chi^2$$ (`r bartlett$parameter`) = `r bartlett$statistic`, p = `r bartlett$p.value(round = 3)`. The Kaiser-Meyer-Olkin measure of sampling adequacy was `r kmo$MSA(round =2)`. The determinant of the correlation matrix was `r det(round = 5)`, indicating no multicollinearity issues.

We conducted our factor analysis on the first sub-sample using the psych package (version #whatever) in R. Parallel analysis indicated we should retain six factors for an exploratory factor analysis, although only three factors had eigenvalues greater than 1.0. As such, we computed solutions for a three and a six factor solution using varimax rotation, deciding to retain items that had at least |.3| factor loading on only one of the resulting factors. The six-factor solution explained 33% of the variance and was preferred because the resulting factor structure was more easily interpretable than the three factor solution, which only explained 25% of the variance.

We retained 29 out of the original 36 items. The retained items, factor loadings, and communalities are all shown in Table #. The first identified factor contains six items that cohere around scientistic perceptions of science, elevating the status of science as superior to other knowledge-production efforts. The second factor identified contains three items that emphasize the role of science within society, focusing on societal consequences of scientific endeavors. The third factor contains eight items that, together, appear to represent participants' level of cynicism regarding the credibility of scientists and the scientific community. The fourth factor contains three items that tap into participants' perceptions of the social power dynamics within the scientific community. The fifth factor identified contains five items that reflect perceptions of science as constrained to a relatively narrow set of methods and practices in order to be counted as science, i.e. a mythological "the" scientific method. The final factor identified contains four items that appear to tap into perceptions of science as capable of pure objectivity.

For cross-validation purposes, we ran a CFA on the second sub-sample using the factor structure extracted from the EFA. We carried out the CFA with maximum likelihood estimation using the lavaan package (version #whatever) in R. Fit indices, unfortunately, revealed the model was insufficient to adequately fit the data, $$\chi^2$$(362) = 1142.995, p < .001, CFI = .612, AGFI = .803, RMSEA = .070, and SRMR = .088. Typically, values exceeding .90 for CFI and AGFI indicate acceptable fit and values below .05 for RMSEA and SRMR indicate acceptable fit (Hu & Bentler, 1999).

Despite results from the CFA, the strong theoretical coherence of the factor structure suggests ways in which future iterations of a scale to tap into peoples' perceived values about science could improve upon the current Values in Science scale. More precise item wording may be necessary to better capture elements of the aforementioned constructs. Similarly, additional items may be needed to capture more aspects of each the proposed latent constructs. Lastly, there may be additional relevant values and perceptions about science that are not captured at all in the present iteration of the Values in Science scale. Future work should consider each of these suggestions to build upon the preliminary work we report here on the development of a psychometrically valid instrument to capture peoples' values in science. Excepting the comparative fit index (CFI), the fit indices were close to thresholds indicative of acceptable model fit. As such, we have decided to continue with our subsequent analyses using the six-factor structure as a means of examining whether participant responses to the Values in Science scale has any potential predictive validity for participant responses to the perceived credibility of scientists who acknowledge the role of values in their scientific research. Results of any such tests must be interpreted with caution, however, in light of the CFA results.
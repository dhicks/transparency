---
title: "Values disclosures and trust in science: A replication study"
# author: 
#     - name: Daniel J. Hicks
#       orcid: 0000-0001-7945-4416
#       email: dhicks4@ucmerced.edu
#       affiliations:
#           - ref: ucm
#       attributes:
#           corresponding: true
#     - name: Emilio Lobato
#       orcid: 0000-0002-3066-2932
#       email: elobato@ucmerced.edu
#       affiliations: 
#         - ref: ucm

affiliations: 
    - id: ucm
      name: University of California, Merced
      address: 5200 N Lake Road
      city: Merced
      state: CA
      postal-code: 95343

abstract: |
  While philosophers of science generally agree that social, political, and ethical values can play legitimate roles in science, there is active debate over whether scientists should disclosure such values in their public communications.  This debate depends, in part, on empirical claims about whether values disclosures might undermine public trust in science.  In a previous study, Elliott et al. used an online experiment to test this empirical claim.  The current paper reports a replication attempt of their experiment. Comparing results of the original study and our replication, we do not find evidence for a transparency penalty or "shared values" effect, but do find evidence that the content of scientific conclusions (whether or not a chemical is found to cause harm) might effect perceived trustworthiness and that scientists who value public health and disclose this value might be perceived as more trustworthy.  

toc: false
execute:
    echo: false
bibliography: |
  transparency-Hicks.yaml
format: 
    html: default
    pdf: default
always_allow_html: true
header-includes:
  - \usepackage[left]{lineno}
  - \linenumbers
---

| Daniel J. Hicks^1\*^, Emilio Lobato^1^
| ^1^: University of California, Merced, 5200 N Lake Road, Merced, CA, 95343
| ^\*^: Address correspondence to <dhicks4@ucmerced.edu>
| 
| Keywords: trust, trust in science, values in science, replication, philosophy of science


```{r}
#| include: false

library(gtsummary)
library(kableExtra)
library(here)
here()
out_dir = here('out')

print_tbl = function(x) {
    x |> 
        as_kable_extra(booktabs = TRUE) |> 
        kable_styling(latex_options = c('scale_down'))
}
```

# Introduction #

Over the past 15 years, many philosophers of science have rejected the ideal of value-free science and related, traditional understandings of objectivity and political neutrality of science [@DouglasSciencePolicyValuefree2009; @ElliottTapestryValuesIntroduction2017].  According to the value-free ideal, social and political values — such as feminism, environmentalism, or the protection of human health — have no legitimate role to play in the evaluation of scientific hypotheses.  The value-free ideal is compatible with allowing social and political values to play important roles earlier and later in inquiry.  Specifically, these values may legitimately shape the content and framing of research questions — researchers might decide to investigate whether chemical X causes cancer out of a concern to protect human health — and will be essential when scientific findings are used to inform public policy — say, banning the use of chemical X.  But, according to the value-free ideal, these values must not influence the collection and analysis of data and the process of reaching an overall conclusion about whether or not chemical X causes cancer.  

Challenges to the value-free ideal argue that at least some social and political values may, or even should, play a role in the evaluation of scientific hypotheses.  Keeping with the example, the question of whether chemical X causes cancer has much more significant social and political implications than the question of whether chemical X fluoresces under ultraviolet light.  So, in the interest of protecting human health, it would be appropriate to *scientifically* accept the hypothesis that chemical X is carcinogenic for regulatory purposes based on relatively weak or provisional evidence [@CranorSocialBenefitsExpedited1995; @ElliottNonepistemicValuesMultiple2014; @FernandezPintoLegitimizingValuesRegulatory2019].  

While this kind of argument indicates that at least some social and political values may or should play at least some role in the evaluation of at least some scientific hypotheses, it doesn't provide us with much positive guidance:  which values, playing which roles, in the evaluation of which hypotheses?  [@HicksNewDirectionScience2014]  One (partial) answer to this set of questions appeals to the ideal of transparency:  scientists should disclose to the public the social and political values that have influenced their research [@ElliottSciencePolicyTransparency2014; @McKaughanBacktrackingEthicsFraming2013].  But some philosophers have objected to this transparency proposal, arguing that it might actually undermine trust in science.  According to this objection, members of the public generally accept the ideal of value-free science; values disclosures would violate this ideal, making scientists (incorrectly) appear biased and untrustworthy [@JohnEpistemicTrustEthics2017; @KovakaClimateChangeDenial2021].  

This objection is an empirical prediction: if scientists disclose their values, they will be perceived as less trustworthy.  @ElliottValuesEnvironmentalResearch2017 conducted an online survey study to evaluate this prediction. The authors found tentative evidence that disclosing values may reduce the perceived trustworthiness of a scientist, and that this effect may be moderated by whether or not participants share the same values as the scientist and whether or not the scientist reports findings contrary to their stated values. However, these authors collected data from a somewhat small sample using Amazon Mechanical Turk, and adopted an analytical approach that diluted their sample across different conditions. In this paper, we report the results of a replication of @ElliottValuesEnvironmentalResearch2017 study 1, using a larger sample and a more statistically efficient analytical approach. The major results found by @ElliottValuesEnvironmentalResearch2017 serve as the basis for our hypotheses, described below. 


# Methods and Materials #

## Experimental design ##

We used the same experimental stimulus and design as @ElliottValuesEnvironmentalResearch2017, which we embedded within a larger project examining the public's perceptions of the values in science. In this paper, we are only reporting the replication component of the project. The replication component was the same 3 (Values: No Disclosure, Economic Growth, Public Health) \times 2 (Harm: Causes Harm, Does Not Cause Harm) experimental design as used by @ElliottValuesEnvironmentalResearch2017.  In each condition, participants are first shown a single presentation slide and the following explanatory text: 

> For several decades, a scientist named Dr. Riley Spence has been doing research on chemicals used in consumer products. One chemical—Bisphenol A, popularly known as BPA—is found in a large number of consumer products and is suspected of posing risks to human health. Yet, scientists do not agree about these possible health risks. Dr. Spence recently gave a public talk in Washington, D.C., about BPA research. Here is the final slide from Dr. Spence's presentation. 

| No other information about the (fictional) Dr. Spence is provided.  The content of the slide varies across the conditions (@fig-stimulus).  Each slide has the header "My conclusion," followed by a list of 2 or 3 bullets.  The first bullet, if present, makes a values disclosure, stating that either economic growth or public health should be a top national priority.  This first bullet is not present in the "no disclosure" values condition.  The second bullet, identical across conditions, states "I examined the scientific evidence on potential health risks of BPA."  The third bullet concludes that BPA either does or does not cause harm to people, depending on whether the subject is in the "causes harm" or "does not cause harm" condition. 

![Example of the experimental stimulus, from the "public health values disclosure" + "causes harm" conclusion.](fig1_stimulus.png){#fig-stimulus}

After viewing the slide, the participant is then asked to rate Dr. Spence's trustworthiness using a semantic differential scale.  @ElliottValuesEnvironmentalResearch2017 used an ad hoc scale; we used the Muenster Epistemic Trustworthiness Inventory [METI\; @HendriksMeasuringLaypeopleTrust2015], which substantially overlaps but is not identical to the @ElliottValuesEnvironmentalResearch2017 scale.  Even though the ad hoc scale created by Elliot and colleagues had acceptable internal reliability, the METI was developed and psychometrically validated to measure the perceived trustworthiness of experts. On the METI, participants rate the target scientist (the fictional Dr. Spence) on a 1-7 scale for 14 semantic differential items, where each item is anchored at the ends by a pair of words, such as competent-incompetent or responsible-irresponsible. The METI scale captures perceived trustworthiness of a target along three dimensions: competence, benevolence, and integrity. However, the three dimensions were strongly correlated in our sample (Pearson r values ranging from .75 to .92). To avoid issues of multicollinearity in our analyses, we averaged participant scores into a single composite measure. In both @ElliottValuesEnvironmentalResearch2017 and our replication analysis, all items were averaged together to form a 1-7 composite measure of trustworthiness.  To aid interpretation, in the analysis the direction of the scale has been set so that increased values correspond to increased perceived trustworthiness.  

Ethicists make a key distinction between trust and trustworthiness [@BaierTrustAntitrust1986].  Trust is a verb: placing one's trust in someone else, with respect to some activity or domain.  Trustworthiness is an assessment, of whether or not that trust is appropriate.  In the experiment, participants might trust Dr. Spence's conclusion even if they judge Spence to be untrustworthy, or vice versa; though we would typically expect these two to go together.  As its name indicates, METI assesses perceived trustworthiness, not trust (such as accepting Dr. Spence's conclusion about BPA).  

@ElliottValuesEnvironmentalResearch2017 explain that they selected BPA as a complex, ongoing, public scientific controversy.  For our replication, we chose to keep BPA, rather than switching to a different public scientific controversy with a higher profile in 2021, such as climate change, police violence, voter fraud, or any of numerous aspects of the COVID-19 pandemic.  All of these controversies are highly politically charged, with prominent public experts and counterexperts [@GoldenbergVaccineHesitancyPublic2021 100-1, ch. 6].  While we anticipated some effects of political partisanship in the BPA case, we felt it would be less likely to swamp the values disclosure that was our primary interest.  

After filling in the METI, participants provided demographic information, including self-identifying their political ideology, and other sections of the survey that are not examined here.  Due to researcher error, a question about the participants' values (whether they prioritize economic growth or public health) was omitted in the first wave of data collection; this question was asked in a followup wave.  

## Replication hypotheses and analytical approach ##

We identified 5 major findings from @ElliottValuesEnvironmentalResearch2017 for our replication attempt:  

H1. Modest correlation between values and ideology
~ \(a\) Political liberals are more likely to prioritize public health over economic growth, compared to political conservatives; but (b) a majority of political conservatives prioritize public health.  

H2. Consumer risk sensitivity
~ Scientists who find that a chemical harms human health are perceived as more trustworthy than scientists who find that a chemical does not cause harm. 

H3. Transparency penalty
~ Scientists who disclose values are perceived as less trustworthy than scientists who do not.  

H4. Shared values
~ Given that the scientist discloses values, if the participant and the scientist share the same values, the scientist is perceived as more trustworthy than if the participant and scientist have discordant values.  

H5. Variation in effects
~ The magnitude of the effects for hypotheses 2-4 vary depending on whether the participant prioritizes public health or economic growth.  

Hypothesis H3, the transparency penalty, corresponds to the objection to transparency:  disclosing values undermines trust in science.  However, the shared values effect works in the opposite direction, counteracting the transparency penalty.  

Hypothesis 1 was analyzed using Spearman rank order correlation to test H1a and a visual inspection of descriptive statistics to test H1b.  Hypotheses 2-5 were analyzed using linear regression models as a common framework, with a direct acyclic graph (DAG) constructed *a priori* to identify appropriate adjustments (covariates) for H4 and 5.  Because @ElliottValuesEnvironmentalResearch2017 made their data publicly available, and our analytical approaches differ somewhat from theirs, we conducted parallel analyses of their data, and report these parallel results for H2 and 3.  Exploratory data analysis was used throughout to support data validation and aid interpretation.  


## Participants ##

Participants were recruited using the online survey platform Prolific, and the survey was administered in a web browser using Qualtrics.  Prolific has an option to draw samples that are balanced to be representative by age, binary gender, and a 5-category race variable (taking values Asian, Black, Mixed, Other, and White) for US adults [@RepresentativeSamplesFAQ2022].  A recent analysis finds that Prolific produces substantially higher quality data than Amazon Mechanical Turk for online survey studies, though three of the five authors are affiliated with Prolific [@PeerDataQualityPlatforms2021].  Preliminary power analysis recommended a sample of approximately 1,000 participants to reliably detect non-interaction effects (H1-4).  

After excluding participants who declined consent after opening the survey or did not complete the survey, we had 988 participants in the full analysis sample ($M_{age}$ = 44-years-old, $SD_{age}$ = 16-years, Woman/Female = 498, Man/Male = 458, White = 712, Black = 124, Asian or Pacific Islander = 63, Hispanic = 33, American Indian or Alaskan Native = 5, Mixed or Other = 51).  Participants were randomly assigned to condition, with 163 assigned to the No Disclosure + Causes Harm condition, 165 assigned to the No Disclosure + Does Not Cause Harm condition, 165 assigned to the Economic Growth + Causes Harm condition, 165 assigned to the Economic Growth + Does Not Cause Harm condition, 168 assigned to the Public Health + Causes Harm condition, and 162 assigned to the Public Health + Does Not Cause Harm condition (@tbl-condition). Due to researcher error a question about participants' values was not included in the original survey. Of the full 988 participants, 844 participants (85%) responded to the followup question about their own values (participant prioritizes economic growth or public health).  Consequently, subsamples for hypotheses 4 and 5 were substantially smaller than the full analysis sample.  

```{r}
#| label: tbl-condition
#| tbl-cap: "Assignment of participants to conditions"
readRDS(here(out_dir, '03_condition_tbl.Rds')) |> 
	print_tbl()
```

The study was approved by the UC Merced IRB on August 17, 2021, and data collection ran October 18-20, 2021. The followup survey asking the initial sample of participants about their own values regarding economic growth and public health was conducted December 8, 2021 through March 5, 2022. 


## Software and reproducibility ##

Data cleaning and analysis was conducted in R version 4.1.2 [@RCoreTeamLanguageEnvironmentStatistical2021], with extensive use of the `tidyverse` suite of packages version 1.3.1 [@WickhamWelcomeTidyverse2019].  Regression tables were generated using the packages `gt` version 0.5.0 [@IannoneGtEasilyCreate2022] and `gtsummary` version 1.6.0 [@SjobergReproducibleSummaryTables2021]. 

Anonymized original data and reproducible code are available at <https://github.com/dhicks/transparency>.  Instructions in that repository explain how to automatically reproduce our analysis. 


# Results #


Critically, our data are unlikely to be representative by education level and political ideology.  In 2021, about 9% of US adults 25 or over had a less than high school education, and 38% had a Bachelor's degree or higher [@CPSHistoricalTime2022 fig. 2].  Only 1% of our participants reported a less than high school education, and 57% reported a Bachelor's degree or higher.  For political ideology, the General Social Survey has consistently found over several decades that about 30% of US adults identify as liberal, about 30% identify as conservative, and about 40% as moderate [@GSSDataExplorer2022].  Among our participants, liberals (574) heavily outnumber conservatives (248; @fig-part-values).  Both overrepresentation of college graduates and underrepresentation of conservatives (especially conservatives with strong anti-institutional views) are known issues in public opinion polling [@KennedyEvaluation2016Election2018].  In exploratory data analysis, we noted that there was essentially no correlation between political ideology and perceived trustworthiness (in the online supplement, see subsection H5-shared).  This was surprising, since general trust/distrust in science has become a partisan phenomenon over the last few decades:  data from the General Social Survey shows increasing trust in science from liberals and decreasing trust from conservatives [@GauchatPoliticizationSciencePublic2012; @LeePartyPolarizationTrust2021], and many (though not all) prominent public scientific controversies align with liberal-conservative partisanship [@FunkAmericansPoliticsScience2015].  

Because of these representation issues, insofar as some political conservatives are both less likely to participate in studies on Prolific (or at least in our particular study) and likely to perceive Spence as less trustworthy, this will produce omitted variable bias for analyses that require adjustment by political ideology.  Analysis of our *a priori* DAG indicated that, for the hypotheses examined here, this adjustment was not necessary in any case.  When some adjustment was necessary (H4-5), we included political ideology along with other demographic variables in an alternative model specification as a robustness check.  In line with the DAG, in no case did these robustness checks produce indications of omitted variable bias.  



## H1: Correlation between values and ideology

We tested the hypothesis of a modest correlation between values and ideology in two ways. First, to test (H1a) whether political liberals are more likely to prioritize public health over economic growth compared to conservatives, we conducted a Spearman's rank order correlation. Results revealed a significant correlation in line with the hypothesis, Spearman's $\rho$ = -.47, *p* < .001. Political liberals were more likely than conservatives to value public health over economic growth. To test (H1b) that a majority of political conservatives priortize public health over economic growth we cross-tabulated the data. Results revealed that, contrary to the hypothesis, slightly more than half of the self-reported political conservatives in our sample reported valuing economic growth (51.7%) over public health (48.3%). See @fig-part-values. 

![Participant values by political ideology. (A) Absolute counts, (B) shares within political ideology categories.  Panel (A) shows that our sample substantially over-represents political liberals.](fig2_part_values.png){#fig-part-values} 

## H2 and H3: Consumer risk sensitivity and transparency penalty

Next, we tested the hypotheses that (H2) scientists who find a chemical harms human health are perceived as more trustworthy than scientist who find that a chemical does not cause harm and (H3) a scientist who discloses values are perceived as less trustworthy than a scientist who does not. For this analysis, we regressed participants' METI ratings onto both the Conclusions and Disclosure experimental conditions. The full model was significant, adj. $R^2$ = .148, *F*(2, 985) = 86.59, *p* < .001 (see @tbl-bc). Specifically, results revealed that the conclusions reported by the scientist predicted participants' perceived trustworthiness in line with our hypothesis. Participants rated the scientist who reported that BPA does not cause harm as less trustworthy ($M_{sd} = 4.48_{1.26}$) than the scientist who reported that BPA causes harm ($M_{sd} = 5.47_{1.12}$), $\beta$ = -0.99, *t*(985) = -13.09, *p* < .001. By contrast, the results do not provide evidence in favor of our hypothesis that a scientist disclosing their values ($M_{sd} = 4.94_{1.33}$) are perceived as less trustworthy than a scientist who does not disclose values  ($M_{sd} = 5.05_{1.21}$), $\beta$ = -0.12, *t*(985) = -1.46, *p* = .143. 

```{r}
#| label: tbl-bc
#| tbl-cap: "Regression analysis for hypotheses H2 (consumer risk sensitivity) and H3 (disclosure effect)"
readRDS(here(out_dir, '03_bc_tbl.Rds')) |> 
    print_tbl()
```

## H4: Shared values

Next, we tested the hypothesis that (H4) if the participant and scientist share the same values, the scientist is perceived as more trustworthy than if the participant and scientist do not share the same values. For this and related analyses, we only included data from participants who were assigned to either of the Disclosure conditions and self-reported their own values, reducing the sample to 567. Following this, we created a new Shared Values variable as a composite of the participants' reported values and the scientist's values. For this analysis, an a priori directed acyclic graphic (DAG) indicated that a univariate regression of participants' METI rating onto Shared Values would produce a biased estimate, that adjustments were required for both the participant's and scientist's values, and that including demographic variables would not effect this estimate (@fig-shared-dag).  While the univariate model was significant, adj. $R^2$= .029, *F*(1, 565) = 18.0, *p* < .001 (@tbl-shared model 1), after adjusting for scientist values Shared Values did not emerge as a significant predictor of participants' perceptions of the trustworthiness of the scientist (@tbl-shared models 2-4), indicating that the univariate estimate was indeed biased.  Rather, only the scientist's disclosed values significantly predicted how trustworthy participants rated the scientist, such that a scientist who disclosed valuing public health was rated as more trustworthy than a scientist who disclosed valuing economic growth.  This scientist values effect was robust across alternative model specifications, with an estimated effect of 0.6 (95% CI 0.4-0.8; @tbl-shared models 2-5), again consistent with the DAG.  

![Directed acyclic graph (DAG) for analysis of the shared values effect.  If the DAG is faithful, adjusting for scientist values (`sci_values`) and participant values (`part_values`) is sufficient for estimating the effect of shared values on perceived trustworthiness (`METI`).  In particular, adjusting for demographics should not change the regression coefficient on shared values.](fig3_shared_values_dag.png){#fig-shared-dag}

```{r}
#| label: tbl-shared
#| tbl-cap: "Sequential regression analysis of hypothesis H4 shared values/scientist values effects"
readRDS(here(out_dir, '03_shared_values_tbl.Rds')) |> 
    print_tbl()
```

## H5: Variation in effects

We ran the following analyses to test our hypothesis that (H5) the magnitude of the effects found for the tests of H2-H4 vary depending on whether the participant prioritizes public health or economic growth. Results are presented in [@tbl-eb; @tbl-ec; @tbl-sci-part].

### Consumer risk sensitivity

To test whether the findings regarding consumer risk sensitivity vary as a function of participants' values, we regressed participants' METI ratings of the scientist in the stimuli onto the Conclusions condition, Participants' Values variable, and the Conclusions by Participants' values interaction term. The full model was significant, adj. $R^2$ = .173, *F*(3, 840) = 59.95, *p* < .001 (@tbl-eb model 2). As with the earlier analysis, results showed a main effect of the Conclusions condition, such that participants rated the scientist who reported that BPA does not cause harm as less trustworthy than the scientist who reported that BPA causes harm, $\beta$ = -0.75, *t*(840) = -4.51, *p* < .001. However, this effect was qualified by a significant interaction with participants' values, $\beta$ = -0.41, *t*(840) = -2.16, *p* = .031. Participants who prioritized public health and read about a scientist who concluded that BPA causes harm rated the scientist as more trustworthy ($M_{sd} = 5.56_{1.09}$) than participants with the same values who read about a scientist who concluded that BPA does not cause harm ($M_{sd} = 4.40_{1.24}$; @fig-conclusion-part). 

```{r}
#| label: tbl-eb
#| tbl-cap: "Regression analysis of H5-consumer, interaction between participant values and consumer risk sensitivity"
readRDS(here(out_dir, '03_eb_tbl.Rds')) |> 
    print_tbl()
```

![Data, group means, and 95% confidence intervals for H5-consumer, interaction of consumer risk sensitivity and participant values. Panels correspond to participant values.](fig4_conclusion_part.png){#fig-conclusion-part}


### Transparency penalty

To test whether the findings above about a hypothesized transparency penalty may vary based on participants' values, we regressed participants' METI ratings of the scientist onto the Disclosure condition variable, Participants' Values variable, and the Disclosure by Participants' Values interaction term. The full model was not significant, adj. $R^2$< .001, *F*(3, 840) = 1.19, *p* = .31 (@tbl-ec model 2). As with the earlier analysis, our results do not provide evidence for a transparency penalty to the perceived trustworthiness of a scientist, either in general or interacting with participants' own reported values, $\beta$ = -0.23, *t*(840) = -1.07, *p* = 0.286. 

```{r}
#| label: tbl-ec
#| tbl-cap: "Regression analysis of H5-transparency, interaction between participant values and transparency penalty"
readRDS(here(out_dir, '03_ec_tbl.Rds')) |> 
    print_tbl()
```



### Shared values and scientist values

Without adjustments, Shared Values appears to have a substantial interaction with Participant Values:  Shared Values appears to increase perceived trustworthiness for participants who value public health, while decreasing perceived trustworthiness for participants who value economic growth (@fig-shared-part).  However, as indicated by our analysis for a potential shared values effect, the estimated effects for both shared values and participant values are biased if the model is not adjusted for scientist values.  (@fig-shared-dag and @tbl-shared show how this bias occurs.  Shared Values is a collider between Participant Values and Scientist Values; hence, if a model specification includes Participant Values and Shared values but not Scientist Values, there is an open path between Participant Values and METI, resulting in a biased estimate for Participant Values.)  Because Shared Values is the logical biconditional of Participant Values and Scientist Values, and the Shared Values $\times$ Participant Values interaction term is their logical conjunction, including all four variables (Participant, Scientist, and Shared Values, along with the interaction term) in a regression model creates perfect collinearity.  

![Data, group means, and 95% confidence intervals for H5-shared, interaction of shared values and participant values.  Panels correspond to participant values.](fig5_shared_part.png){#fig-shared-part}

As we found above, rather than a shared values effect there appears to be an effect of scientist values.  Replotting the same data as @fig-shared-part based on scientist values, rather than shared values, suggests a more consistent effect across participant values (@fig-sci-part). Therefore, we conducted an unplanned post hoc analysis to test whether an effect of scientist values might vary as a function of participants values.  We regressed participants' METI of the scientist in the stimulus onto the Scientist Values variable, Participants' Values variable, and their interaction (@tbl-sci-part). This model was significant, adj. $R^2$ = 0.065, *F*(3, 563) = 14.15, *p* < .001. The estimate for the scientist values term was significant, $\beta$ = 0.53, *t*(563) = 2.26, *p* = 0.02, while the estimate for the interaction term was not, $\beta$ = 0.20, *t*(563) = 0.75, *p* = 0.45.  Estimates for both variables had large uncertainties, with confidence intervals of about 1 (95% confidence intervals, scientist values: 0.07, 1.0; interaction term: -0.32, 0.72).  

![Data, group means, and 95% confidence intervals for a potential interaction of scientist values and participant values. Panels correspond to participant values. This figure is identical to @fig-shared-part, except the columns in the economic growth panel have been switched.](fig6_sci_part.png){#fig-sci-part}

```{r}
#| label: tbl-sci-part
#| tbl-cap: "Regression analysis of interaction between scientist values and participant values"
readRDS(here(out_dir, '03_sci_part_tbl.Rds')) |> 
    print_tbl()
```


# Conclusion #

@tbl-summary summarizes the results of our replication attempts. We were able to successfully replicate the correlation between political ideology and valuing public health and consumer risk sensitivity (a scientist who concludes BPA is harmful is perceived as more trustworthy), and in addition found evidence of an interaction between consumer risk sensitivity and whether the participant values public health (participants who value public health have more extreme reactions to the scientist's conclusion).  

We did not find evidence for a transparency penalty or shared values effect.  And we found evidence of a scientist values effect (a scientist who discloses valuing public health is perceived as more trustworthy), which was not identified by @ElliottValuesEnvironmentalResearch2017.  

{{< include summary_table.md >}}


@ElliottValuesEnvironmentalResearch2017 claim evidence for a shared values effect by comparing values disclosure vs. no disclosure (that is, estimating a transparency effect) in quasi-independent subsamples across a 2 $\times$ 2 $\times$ 2 design (participant values $\times$ scientist conclusion $\times$ scientist values; @ElliottValuesEnvironmentalResearch2017 fig. 1).  This approach is difficult to interpret; their reasoning seems to be that this estimate is statistically significant in 4/8 cells, and in 3/4 of these cells (1 where respondent values economic growth) the participant and scientist share values.  But these analyses are underpowered (for example, sample sizes are well below 100 for some of the cells in which participants value economic growth) and the overall approach cannot distinguish a shared values effect from other potential effects (scientist values, participant values).  Applying our regression analysis approach to the data published by @ElliottValuesEnvironmentalResearch2017, the estimate for shared values was not statistically significant, $\beta$ = 0.29, *t*(335) = 1.75, *p* = 0.08; while the estimate for scientist values was statistically significant, $\beta$ = 0.41, *t*(335) = 2.48, *p* = 0.014.  It seems likely that claims of a shared values effect could have been the result of a less appropriate analytic approach given the nature of the data. 

The disagreement over a transparency penalty is more difficult to explain.  Using our approach and the data from @ElliottValuesEnvironmentalResearch2017, the estimate for a values disclosure was statistically significant (@tbl-bc model 1).  One possible explanation is that, over the course of the COVID-19 pandemic, members of the general public have become used to "scientists" (including both bench researchers and public health officials) making claims about the importance of protecting public health.  A values disclosure that might have been regarded as violating the value-free ideal, pre-pandemic, might now be seen as routine.  

The scientist values effect — a scientist who discloses valuing public health is perceived as more trustworthy — might be interpreted as supporting some of the claims of the "aims approach" in philosophy of science.  This approach argues that scientific fields often have both social or practical aims, along with epistemic aims such as the pursuit of truth [@ElliottNonepistemicValuesMultiple2014; @IntemannDistinguishingLegitimateIllegitimate2015; @PotochnikIdealizationAimsScience2017; @FernandezPintoLegitimizingValuesRegulatory2019; @HicksWhenVirtuesAre2022].  For example, the field of public health might have the practical aim of promoting the health of the public.  Where certain values are seen as conflicting with these aims, it is morally wrong for a scientist to promote those values.  Rather than violating the value-free ideal, a scientist who discloses valuing public health might be seen as following the norms of the field of public health.  

## Limitations

As noted above, a major limitation of the current study is that the sample substantially underrepresents political conservatives.  Our *a priori* DAG analysis and empirical robustness checks do not indicate any bias in our reported estimates due to this representation problem.  However, our estimates could still be biased if any of the effects examined here have interactions with political ideology.  

Another limitation is that our sample only considers US adults.  It does not include residents of any other country, and does not track where our participants might live within the US.  Public scientific controversies often have different dynamics both across national borders and within different regions of geographically large countries [@MilesPublicPerceptionScientific2003; @HoweGeographicVariationOpinions2015; @MildenbergerDistributionClimateChange2016; @MildenbergerSpatialDistributionRepublican2017; @SturgisTrustScienceSocial2021]. 

A third limitation is that the stimulus only considers a single controversy, over the safety of the chemical bisphenol A (BPA).  Public attention to this controversy peaked around 2009.  While it remains unsettled in terms of both science and policy, it is much less socially and politically salient than controversies over topics such as climate change or vaccination. This limitation affects our ability to speculate about the generalizability of our findings. That is, the effects we reported may only apply narrowly to a subset of sociopolitically controversial scientific topics that have not achieved the degree of salience or persistence in sociopolitical discourse for as long as topics like climate change or vaccinations.

## Directions for future work

The design from @ElliottValuesEnvironmentalResearch2017 assesses public perceptions of the value-free ideal indirectly, by probing whether violations of this ideal would lead to decreased perceived trustworthiness.  But do members of the general public accept the value-free ideal?  Concurrently with this replication study, we also asked participants directly for their views on the value-free ideal and related issues and arguments from the philosophy of science literature.  A manuscript discussing this effort to develop a "values in science scale" is currently under preparation.  

We suggest three directions for future work in this area, two in the experimental stimulus and one in the endpoint or outcome measured. 

The stimulus developed by @ElliottValuesEnvironmentalResearch2017 involves a single (fictional) scientist.  But public scientific controversies often feature conflicting claims made by contesting experts and counterexperts.  For example, as the Omicron wave of the COVID-19 pandemic waned in the US in spring 2022, various physicians, public health experts, science journalists, and government officials made conflicting claims about the effective severity of Omicron, the relative effectiveness of masks in highly vaccinated populations, and the need to "return to normal" [@Adler-BellPandemicInterpreter2022; @KhullarWillCoronavirusPandemic2022; @YongHowDidThis2022].  For members of the general public, the question was not whether to trust the claims of a given expert, but instead which experts to trust.  Publics might perceive both expert A and expert B to be highly trustworthy, but would need to make a decision about who to trust if these experts are making conflicting claims.  It would be straightforward to modify the single-scientist stimulus to cover this kind of "dueling experts" scenario.  To modify METI, participants might be asked which of the two experts they would consider more competent, ethical, honest, etc.  (Public scientific controversies can include other prominent actors with no relevant expertise, such as political journalists, elected officials, or social media conspiracy theorists.  Such individuals might nonetheless be treated by publics as trusted sources for factual claims.  Understanding why many people might trust a figure like Alex Jones over someone like Anthony Fauci would likely require prior work disentangling trustworthiness, expertise, and institutional standing.)  

@BrownTrustExpertiseScientific2022 emphasized a distinction between individuals, groups, and institutions, as both trustors and trustees, and argued that much of the philosophical and empirical research on trust in science has focused on either individual-individual trust (as in @ElliottValuesEnvironmentalResearch2017) or individual-group trust (as in the General Social Survey, which asks individual respondents about their confidence in "the scientific community").  In future experiments, "Dr. Riley Spence" could be represented with an institutional affiliation, such as "Dr. Riley Spence of the Environmental Protection Agency" or "Dr. Riley Spence of the American Petroleum Institute" Or claims could be attributed directly to institutions, such as "Environmental Protection Agency scientists" or "a report published by the American Petroleum Institute." 

As noted above, ethicists make a distinction between trust and trustworthiness.  Instruments such as METI assess trustworthiness: whether a speaker is perceived to have qualities that would make it appropriate to trust them.  Trust itself is closer to behavior than attitudes, and would probably be better measured (in online survey experiments and similar designs) by asking whether participants accept claims made by speakers or support policy positions endorsed by speakers.  In addition, research on the climate controversy suggests that acceptance of scientific claims might underpredict policy support.  Since 2008, public opinion studies by the Yale Program on Climate Change Communication have found that only about 50-60% of the US public understands that there's a scientific consensus on climate change, that this understanding is politically polarized (conservatives are substantially less likely to recognize the consensus), but also that some climate policies enjoy majority support even among conservatives [@LeiserowitzPoliticsGlobalWarming2022].  Putting these points together, future research could ask participants whether they would support, for example, increased regulation of BPA, either instead of or along with assessing the perceived trustworthiness of the scientific expert.  

# Acknowledgments #

For feedback on this project, thanks to Kevin Elliott and participants at the Values in Science conference and Political Philosophy and Values in Medicine, Science, and Technology 2022 conference.  

# Funding #

Funding for this project was provided by the University of California, Merced.  

# References #
